{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e7e7d9-7801-4c8d-98f9-d6a072f2ac2a",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results.\n",
    "\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups. To ensure the validity of ANOVA results, certain assumptions need to be met:\n",
    "\n",
    "Independence: The observations within each group are independent of each other. Violations of this assumption can occur when there is dependence or correlation among the observations. For example, if the same individuals are included in multiple groups or if there is clustering within groups, the independence assumption may be violated.\n",
    "\n",
    "Normality: The data within each group follow a normal distribution. Deviations from normality can impact the validity of ANOVA. However, ANOVA is somewhat robust to violations of normality, especially when the sample sizes are large. Transformations or non-parametric alternatives may be considered if the normality assumption is severely violated.\n",
    "\n",
    "Homogeneity of variance: The variances of the groups being compared are equal. This assumption is known as homoscedasticity. Violations of this assumption, called heteroscedasticity, can affect the accuracy of ANOVA results. It can lead to inflated or deflated Type I error rates and impact the interpretation of the F-test.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results include:\n",
    "\n",
    "Outliers: Extreme values that do not follow the underlying distribution can affect normality assumptions and lead to skewed results.\n",
    "Unequal variances: When the variances differ significantly between groups, the assumption of homogeneity of variance is violated. This can impact the F-test and make the results less reliable.\n",
    "Non-independence: If the observations within groups are not truly independent, such as repeated measures or nested designs, the assumption of independence is violated, which can invalidate the ANOVA results.\n",
    "When these assumptions are violated, alternative approaches such as non-parametric tests or data transformations may be considered to analyze the data appropriately.\n",
    "\n",
    "\n",
    "\n",
    "Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "\n",
    "The three types of ANOVA are:\n",
    "\n",
    "One-Way ANOVA: This type of ANOVA is used when comparing the means of three or more independent groups or treatments. It examines whether there are significant differences among the means of the groups. One-Way ANOVA is appropriate when there is a single categorical independent variable (factor) and a continuous dependent variable. For example, it can be used to compare the average test scores of students from different schools (groups) or the effects of different doses of a medication (treatments) on patient outcomes.\n",
    "\n",
    "Two-Way ANOVA: Two-Way ANOVA is used to analyze the effects of two independent categorical variables (factors) on a continuous dependent variable. It assesses whether there are significant main effects of each factor and an interaction effect between the factors. For example, it can be used to examine the effects of both gender and age group on exam performance, where gender and age group are the two factors.\n",
    "\n",
    "Repeated Measures ANOVA: Repeated Measures ANOVA is used when the same subjects are measured on the same dependent variable under different conditions or at multiple time points. It is designed to analyze within-subject differences and assess the effects of the independent variables. For example, it can be used to analyze the effects of a drug intervention on blood pressure measured at multiple time points within the same individuals.\n",
    "\n",
    "It's important to choose the appropriate type of ANOVA based on the study design and research question to ensure the analysis aligns with the data and objectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "The partitioning of variance in ANOVA refers to the decomposition of the total variance in a dataset into different components associated with different sources of variation. Understanding this concept is crucial in ANOVA because it allows us to quantify and understand the contributions of different factors to the overall variability in the data.\n",
    "\n",
    "The partitioning of variance in ANOVA consists of three components:\n",
    "\n",
    "Between-Groups Variance (SSB): This component represents the variability among the group means. It quantifies the differences between the groups being compared and reflects the effect of the independent variable (or factors) on the dependent variable.\n",
    "\n",
    "Within-Groups Variance (SSW): This component represents the variability within each group. It captures the random variability or error not accounted for by the independent variable(s). It reflects the natural variability within groups and measurement error.\n",
    "\n",
    "Total Variance (SST): This component represents the overall variability in the data. It is the sum of the between-groups variance and the within-groups variance. It provides a baseline measure of the total variability without considering any specific factor.\n",
    "\n",
    "By understanding the partitioning of variance, researchers can assess the significance of the effects of the independent variable(s) on the dependent variable. The ratio of the between-groups variance to the within-groups variance is used to calculate the F-statistic, which is compared to a critical value to determine if there are significant differences among the group means. Additionally, understanding the partitioning of variance allows researchers to quantify the proportion of variability explained by the independent variable(s) and assess the strength of the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b07c69-2bc0-4263-95a9-f480355be2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 130.0\n",
      "SSE: 130.0\n",
      "SSR: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Q4: To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using   # # # Python, you can use the scipy.stats module:\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "group1 = [2, 4, 6, 8, 10]\n",
    "group2 = [1, 3, 5, 7, 9]\n",
    "group3 = [0, 2, 4, 6, 8]\n",
    "\n",
    "# Concatenate the groups\n",
    "data = group1 + group2 + group3\n",
    "\n",
    "# One-way ANOVA\n",
    "fvalue, pvalue = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Calculate the sum of squares\n",
    "mean = sum(data) / len(data)\n",
    "SST = sum((x - mean)**2 for x in data)\n",
    "SSE = sum((x - mean)**2 for x in group1) + sum((x - mean)**2 for x in group2) + sum((x - mean)**2 for x in group3)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"SST:\", SST)\n",
    "print(\"SSE:\", SSE)\n",
    "print(\"SSR:\", SSR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b919eb-0e7c-4369-a16e-9244420f5d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effect of Group1: 0.899999999999997\n",
      "Main effect of Group2: 3.1083402024861826\n",
      "Interaction effect: 0.04786553023532976\n"
     ]
    }
   ],
   "source": [
    "#Q5: To calculate the main effects and interaction effects in a two-way ANOVA using Python, you can utilize libraries such as statsmodels or scipy.stats. Here's an example using statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "group1 = [2, 4, 6, 8, 10]\n",
    "group2 = [1, 3, 5, 7, 9]\n",
    "values = [5, 8, 7, 3, 6]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Group1': group1,\n",
    "                     'Group2': group2,\n",
    "                     'Values': values})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Values ~ Group1 + Group2 + Group1:Group2', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Extract the main effects and interaction effect\n",
    "main_effect_group1 = anova_table.loc['Group1', 'sum_sq']\n",
    "main_effect_group2 = anova_table.loc['Group2', 'sum_sq']\n",
    "interaction_effect = anova_table.loc['Group1:Group2', 'sum_sq']\n",
    "\n",
    "print(\"Main effect of Group1:\", main_effect_group1)\n",
    "print(\"Main effect of Group2:\", main_effect_group2)\n",
    "print(\"Interaction effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6629b-d07c-4805-ab6a-490aad3d1b1d",
   "metadata": {},
   "source": [
    "Q6: With an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, you can conclude that there is evidence of a statistically significant difference between the groups. The F-statistic represents the ratio of the between-groups variability to the within-groups variability. A higher F-statistic suggests a larger difference between the groups' means compared to the variability within each group.\n",
    "\n",
    "The p-value of 0.02 indicates that there is a 2% chance of observing such a large F-statistic by chance alone if the null hypothesis (no difference between group means) were true. Since the p-value is below the commonly used significance level of 0.05, you reject the null hypothesis.\n",
    "\n",
    "In interpretation, you can state that there is strong evidence to suggest that the mean values of the groups are different from each other. However, it does not provide information on which specific group means differ. Further post-hoc tests or pairwise comparisons can be conducted to identify the specific group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a2714-4e62-4d04-88a4-b8e8690b9ba7",
   "metadata": {},
   "source": [
    "#Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?\n",
    "Handling missing data in a repeated measures ANOVA requires careful consideration, as it can impact the validity and reliability of the results. Here are a few common methods for handling missing data in a repeated measures ANOVA:\n",
    "\n",
    "Complete Case Analysis (CCA): This approach involves excluding any cases with missing data from the analysis. It assumes that the missing data are missing completely at random (MCAR). However, CCA can lead to biased estimates and reduced statistical power if the missingness is related to the variables under study.\n",
    "\n",
    "Pairwise Deletion: In this approach, missing values are ignored on a pairwise basis, allowing each participant to contribute data for the available time points. It assumes that the missing data are missing at random (MAR). Pairwise deletion can lead to biased estimates if the missingness is related to the outcome variable.\n",
    "\n",
    "Imputation Methods: Imputation involves replacing missing values with estimated values based on available information. Common imputation methods include mean imputation, regression imputation, and multiple imputation. Imputation assumes that the data are missing at random (MAR) or missingness can be adequately accounted for through the imputation model. However, imputation can introduce additional uncertainty and may affect the distributional properties of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec244e-1a63-4b31-9196-e22772eb36e8",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "\n",
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific group differences are significant. Some common post-hoc tests include:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD): Tukey's HSD test compares all possible pairs of means and controls the experiment-wise error rate. It is suitable when you have equal sample sizes and want to test all pairwise differences.\n",
    "\n",
    "Bonferroni correction: The Bonferroni correction adjusts the significance level for each pairwise comparison to maintain a desired experiment-wise error rate. It is suitable when you have unequal sample sizes or want to control the family-wise error rate.\n",
    "\n",
    "Sidak correction: Similar to the Bonferroni correction, the Sidak correction adjusts the significance level for multiple comparisons to maintain the desired overall error rate. It is less conservative than Bonferroni and suitable for large sample sizes.\n",
    "\n",
    "Fisher's Least Significant Difference (LSD): The LSD test compares individual pairs of means and is suitable when you have unequal sample sizes. It can be less conservative than Tukey's HSD but does not control the experiment-wise error rate.\n",
    "\n",
    "Scheffé's method: Scheffé's method is a conservative post-hoc test that can be used for both planned and unplanned comparisons. It controls the family-wise error rate but may have lower power compared to other tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ec4fb13-6e78-428f-b0e5-e4604610f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 4.477611940298511\n",
      "p-value: 0.02092299506553578\n"
     ]
    }
   ],
   "source": [
    "#Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "# 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "# to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "# Report the F-statistic and p-value, and interpret the results\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "diet_a = [4, 6, 5, 7, 3, 5, 8, 6, 4, 5]\n",
    "diet_b = [5, 7, 6, 8, 4, 6, 9, 7, 5, 6]\n",
    "diet_c = [6, 8, 7, 9, 5, 7, 10, 8, 6, 7]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Diet': ['A'] * 10 + ['B'] * 10 + ['C'] * 10,\n",
    "                     'WeightLoss': diet_a + diet_b + diet_c})\n",
    "\n",
    "# Fit the one-way ANOVA model\n",
    "model = ols('WeightLoss ~ Diet', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Extract the F-statistic and p-value\n",
    "f_statistic = anova_table['F'][0]\n",
    "p_value = anova_table['PR(>F)'][0]\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66b937a0-cd20-4709-8c8c-fb334f0403c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -3.7472377886619577\n",
      "p-value: 0.0014749928907291047\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other.\"\"\"\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "control_group = [78, 85, 82, 90, 87, 83, 80, 88, 84, 86]\n",
    "experimental_group = [85, 92, 89, 95, 88, 91, 90, 93, 87, 89]\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3fb65-bbfd-445c-b6c9-54bf8cf3ed17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
